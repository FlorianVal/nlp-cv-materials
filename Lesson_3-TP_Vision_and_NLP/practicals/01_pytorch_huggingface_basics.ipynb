{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# TP 3 - Partie 1 : Fondamentaux PyTorch & HuggingFace ü§ó\n",
    "\n",
    "Dans ce notebook, nous allons explorer les briques de base du deep learning avec PyTorch et d√©couvrir l'√©cosyst√®me HuggingFace.\n",
    "\n",
    "**Objectifs :**\n",
    "1. Comprendre ce qu'est un tenseur et comment repr√©senter des donn√©es (images, texte)\n",
    "2. Comprendre ce qu'est une couche Linear et comment s'empilent les couches\n",
    "3. Explorer l'architecture d'un vrai mod√®le de NLP\n",
    "4. Fine-tuner un mod√®le l√©ger sur une t√¢che de classification\n",
    "\n",
    "‚ö†Ô∏è **Contrainte mat√©rielle** : Nous utilisons des mod√®les l√©gers adapt√©s aux PCs de facult√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Imports et setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# V√©rifier le device disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device utilis√© : {device}\")\n",
    "print(f\"Version PyTorch : {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tenseurs",
   "metadata": {},
   "source": [
    "## 2. Les Tenseurs : la brique de base du Deep Learning\n",
    "\n",
    "Un **tenseur** est une structure de donn√©es multi-dimensionnelle. C'est la repr√©sentation universelle en deep learning.\n",
    "\n",
    "**Analogie avec les cours pr√©c√©dents :**\n",
    "- **Image** = Tenseur 3D (Hauteur √ó Largeur √ó Canaux) ou (Canaux √ó Hauteur √ó Largeur)\n",
    "- **Texte tokenis√©** = Tenseur 1D (liste d'indices de tokens)\n",
    "- **Batch d'images** = Tenseur 4D (Batch √ó Canaux √ó H √ó W)\n",
    "\n",
    "### 2.1 Cr√©ation et manipulation de tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor_basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tenseur simple\n",
    "scalar = torch.tensor(42)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print(f\"Scalaire : {scalar}, shape : {scalar.shape}\")\n",
    "print(f\"Vecteur : {vector}, shape : {vector.shape}\")\n",
    "print(f\"Matrice :\\n{matrix}, shape : {matrix.shape}\")\n",
    "\n",
    "# Tenseurs al√©atoires (comme les poids initialis√©s d'un r√©seau)\n",
    "random_tensor = torch.randn(3, 4)  # 3 lignes, 4 colonnes, distribution normale\n",
    "print(f\"\\nTenseur al√©atoire (3√ó4) :\\n{random_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image_as_tensor",
   "metadata": {},
   "source": [
    "### 2.2 Une image est un tenseur d'entiers !\n",
    "\n",
    "T√©l√©chargeons une image simple et regardons sa repr√©sentation en tant que tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image_tensor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger une image exemple\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/300px-PNG_transparency_demonstration_1.png\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Convertir en tenseur PyTorch\n",
    "# Une image RGB a 3 canaux : Rouge, Vert, Bleu\n",
    "img_array = np.array(img)\n",
    "img_tensor = torch.from_numpy(img_array)\n",
    "\n",
    "print(f\"Shape de l'image (H√óW√óC) : {img_tensor.shape}\")\n",
    "print(f\"Type de donn√©es : {img_tensor.dtype}\")\n",
    "print(f\"Valeurs min/max : {img_tensor.min()} / {img_tensor.max()}\")\n",
    "\n",
    "# Affichons quelques pixels\n",
    "print(f\"\\nPixels du coin sup√©rieur gauche (10√ó10) :\")\n",
    "print(img_tensor[:10, :10, 0])  # Canal rouge uniquement\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_tensor.numpy())\n",
    "plt.title(\"Image originale\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_tensor[:, :, 0].numpy(), cmap='Reds')\n",
    "plt.title(\"Canal Rouge uniquement\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüëâ Chaque pixel est juste un nombre entier entre 0 et 255 !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear_layer",
   "metadata": {},
   "source": [
    "## 3. La couche Linear (Fully Connected / Dense)\n",
    "\n",
    "**Rappel du cours :** Une couche Linear (ou fully connected) est la transformation la plus simple d'un r√©seau de neurones.\n",
    "\n",
    "**Formule :** `y = x @ W^T + b`\n",
    "\n",
    "- **x** : entr√©e de dimension `(batch_size, in_features)`\n",
    "- **W** : matrice de poids de dimension `(out_features, in_features)`\n",
    "- **b** : vecteur de biais de dimension `(out_features,)`\n",
    "- **y** : sortie de dimension `(batch_size, out_features)`\n",
    "\n",
    "### 3.1 Cr√©ons une couche Linear from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres de notre couche\n",
    "in_features = 4   # Dimension d'entr√©e\n",
    "out_features = 3  # Dimension de sortie\n",
    "\n",
    "# Cr√©ation d'une couche Linear avec PyTorch\n",
    "linear_layer = nn.Linear(in_features, out_features)\n",
    "\n",
    "print(\"=== Architecture de la couche ===\")\n",
    "print(f\"Entr√©e : {in_features} dimensions\")\n",
    "print(f\"Sortie : {out_features} dimensions\")\n",
    "print(f\"\\nNombre total de param√®tres : {sum(p.numel() for p in linear_layer.parameters())}\")\n",
    "\n",
    "print(\"\\n=== Poids (W) ===\")\n",
    "print(f\"Shape : {linear_layer.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Valeurs :\\n{linear_layer.weight}\")\n",
    "\n",
    "print(\"\\n=== Biais (b) ===\")\n",
    "print(f\"Shape : {linear_layer.bias.shape}\")  # (out_features,)\n",
    "print(f\"Valeurs : {linear_layer.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passons une entr√©e √† travers la couche\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])  # 1 √©chantillon, 4 features\n",
    "print(f\"Entr√©e x : {x}, shape : {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "y = linear_layer(x)\n",
    "print(f\"\\nSortie y : {y}, shape : {y.shape}\")\n",
    "\n",
    "# V√©rifions manuellement le calcul\n",
    "# y = x @ W^T + b\n",
    "manual_y = x @ linear_layer.weight.T + linear_layer.bias\n",
    "print(f\"\\nCalcul manuel : {manual_y}\")\n",
    "print(f\"R√©sultats identiques ? {torch.allclose(y, manual_y)}\")\n",
    "\n",
    "# Visualisation de la transformation\n",
    "print(f\"\\nüìä Transformation : {in_features}D ‚Üí {out_features}D\")\n",
    "print(f\"   Chaque sortie est une combinaison lin√©aire de toutes les entr√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_layer",
   "metadata": {},
   "source": [
    "### 3.2 Empiler des couches : cr√©ation d'un MLP simple\n",
    "\n",
    "Un r√©seau de neurones = empilement de couches avec des fonctions d'activation entre elles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition d'un MLP (Multi-Layer Perceptron) simple\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()  # Fonction d'activation non-lin√©aire\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instancier le mod√®le\n",
    "model = SimpleMLP(input_dim=10, hidden_dim=20, output_dim=2)\n",
    "\n",
    "print(\"=== Architecture du MLP ===\")\n",
    "print(model)\n",
    "\n",
    "# Compter les param√®tres par couche\n",
    "print(\"\\n=== Param√®tres par couche ===\")\n",
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:20s} : {list(param.shape)} ‚Üí {param.numel()} param√®tres\")\n",
    "    total += param.numel()\n",
    "print(f\"\\nTotal : {total} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_exploration",
   "metadata": {},
   "source": [
    "## 4. Exploration d'un vrai mod√®le HuggingFace\n",
    "\n",
    "Maintenant, chargeons un mod√®le de NLP l√©ger et explorons son architecture couche par couche.\n",
    "\n",
    "**Mod√®le choisi :** `distilbert-base-uncased-finetuned-sst-2-english` (DistilBERT small, ~66M params)\n",
    "\n",
    "C'est une version all√©g√©e de BERT, parfaite pour les PCs de facult√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le l√©ger pr√©-entra√Æn√©\n",
    "# DistilBERT = version all√©g√©e de BERT (40% moins de params, 60% plus rapide)\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # Mod√®le de base sans fine-tuning\n",
    "\n",
    "print(\"Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Chargement du mod√®le...\")\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Compter les param√®tres totaux\n",
    "total_params = sum(p.numel() for p in bert_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in bert_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le charg√© : {model_name}\")\n",
    "print(f\"   Param√®tres totaux : {total_params:,} (~{total_params/1e6:.1f}M)\")\n",
    "print(f\"   Param√®tres entra√Ænables : {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer l'architecture du mod√®le\n",
    "print(\"=== Architecture compl√®te ===\")\n",
    "print(bert_model)\n",
    "\n",
    "print(\"\\n=== Structure hi√©rarchique ===\")\n",
    "for name, module in bert_model.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"\\nüìÅ {name}\")\n",
    "    print(f\"   Type : {module.__class__.__name__}\")\n",
    "    print(f\"   Param√®tres : {params:,}\")\n",
    "    \n",
    "    # Afficher les sous-modules (premier niveau)\n",
    "    for sub_name, sub_module in list(module.named_children())[:3]:  # Limiter √† 3\n",
    "        sub_params = sum(p.numel() for p in sub_module.parameters())\n",
    "        print(f\"   ‚îî‚îÄ‚îÄ {sub_name} : {sub_module.__class__.__name__} ({sub_params:,} params)\")\n",
    "    \n",
    "    if len(list(module.named_children())) > 3:\n",
    "        print(f\"   ‚îî‚îÄ‚îÄ ... et {len(list(module.named_children())) - 3} autres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer en d√©tail les poids d'une couche d'attention\n",
    "print(\"=== Deep Dive : Premi√®re couche d'attention ===\")\n",
    "\n",
    "# Acc√©der √† la premi√®re couche du transformer\n",
    "first_layer = bert_model.transformer.layer[0]\n",
    "\n",
    "print(f\"Structure de la couche 0 :\")\n",
    "print(first_layer)\n",
    "\n",
    "# Explorer les poids de l'attention\n",
    "attention = first_layer.attention\n",
    "print(\"\\n=== Poids de l'attention ===\")\n",
    "\n",
    "for name, param in attention.named_parameters():\n",
    "    print(f\"\\nüîß {name}\")\n",
    "    print(f\"   Shape : {param.shape}\")\n",
    "    print(f\"   Valeurs (premiers √©l√©ments) : {param.flatten()[:5].tolist()}\")\n",
    "    print(f\"   Statistiques : mean={param.mean():.4f}, std={param.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward_pass_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer une phrase √† travers le mod√®le et observer les shapes\n",
    "text = \"Deep learning is fascinating!\"\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Texte : '{text}'\")\n",
    "print(f\"\\nTokens : {inputs['input_ids'][0].tolist()}\")\n",
    "print(f\"Token strings : {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# Passage dans le mod√®le\n",
    "with torch.no_grad():  # Pas besoin de gradients pour l'inf√©rence\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "print(f\"\\n=== Shapes des sorties ===\")\n",
    "print(f\"Derni√®re couche cach√©e : {outputs.last_hidden_state.shape}\")\n",
    "print(f\"   ‚Üí [batch_size={outputs.last_hidden_state.shape[0]}, \")\n",
    "print(f\"      seq_len={outputs.last_hidden_state.shape[1]}, \")\n",
    "print(f\"      hidden_dim={outputs.last_hidden_state.shape[2]}]\")\n",
    "\n",
    "print(f\"\\nExplication :\")\n",
    "print(f\"   - Batch size : 1 (une seule phrase)\")\n",
    "print(f\"   - Seq len : {outputs.last_hidden_state.shape[1]} tokens\")\n",
    "print(f\"   - Hidden dim : 768 (dimension des embeddings de DistilBERT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finetuning",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning sur une t√¢che de classification\n",
    "\n",
    "Nous allons fine-tuner DistilBERT sur le dataset SST-2 (sentiment analysis).\n",
    "\n",
    "**Important :** Nous utilisons un sous-ensemble du dataset pour que l'entra√Ænement soit rapide sur CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset SST-2 (petit, rapide √† t√©l√©charger)\n",
    "print(\"Chargement du dataset SST-2...\")\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "print(f\"\\nStructure du dataset :\")\n",
    "print(dataset)\n",
    "\n",
    "# R√©duire la taille pour l'entra√Ænement rapide\n",
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(500))  # 500 exemples\n",
    "small_val = dataset[\"validation\"].shuffle(seed=42).select(range(100))  # 100 exemples\n",
    "\n",
    "print(f\"\\nSous-ensemble pour l'entra√Ænement :\")\n",
    "print(f\"   Train : {len(small_train)} exemples\")\n",
    "print(f\"   Validation : {len(small_val)} exemples\")\n",
    "\n",
    "# Exemple\n",
    "print(f\"\\nExemple d'entr√©e :\")\n",
    "print(f\"   Texte : {small_train[0]['sentence']}\")\n",
    "print(f\"   Label : {small_train[0]['label']} ({'positif' if small_train[0]['label'] == 1 else 'n√©gatif'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser le dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = small_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = small_val.map(tokenize_function, batched=True)\n",
    "\n",
    "# Charger le mod√®le pour la classification\n",
    "model_clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(f\"Mod√®le de classification charg√©\")\n",
    "print(f\"   Classes : n√©gatif (0), positif (1)\")\n",
    "print(f\"   Classifier head : {model_clf.classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,  # 2 epochs pour aller vite\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Ne pas sauvegarder pour gagner du temps\n",
    "    report_to=\"none\",  # Pas de wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Data collator pour le padding dynamique\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Cr√©er le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_clf,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"D√©marrage de l'entra√Ænement...\")\n",
    "print(\"(Cela peut prendre 2-5 minutes sur CPU)\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester sur quelques phrases\n",
    "test_sentences = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I hate this film, it's terrible.\",\n",
    "    \"The acting was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"=== Pr√©dictions ===\")\n",
    "model_clf.eval()\n",
    "for sentence in test_sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_clf(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred].item()\n",
    "    \n",
    "    sentiment = \"positif\" if pred == 1 else \"n√©gatif\"\n",
    "    print(f\"\\n'{sentence}'\")\n",
    "    print(f\"   ‚Üí Sentiment : {sentiment} (confiance : {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "## üéØ R√©capitulatif\n",
    "\n",
    "Dans ce notebook, nous avons vu :\n",
    "\n",
    "1. **Les tenseurs** : Images = tenseurs d'entiers (H√óW√óC), Texte = tenseurs d'indices\n",
    "2. **Les couches Linear** : Transformation lin√©aire `y = x @ W^T + b`\n",
    "3. **L'architecture** : Empilement de couches (Linear ‚Üí Activation ‚Üí Linear...)\n",
    "4. **Exploration de mod√®le** : Inspection des poids et des shapes dans DistilBERT\n",
    "5. **Fine-tuning** : Adaptation d'un mod√®le pr√©-entra√Æn√© √† une nouvelle t√¢che\n",
    "\n",
    "**Prochaine √©tape** : Partie 2 - Transfer Learning en Vision avec des mod√®les l√©gers !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## ‚úèÔ∏è Exercices optionnels\n",
    "\n",
    "1. **Modifier l'architecture** : Ajouter une couche cach√©e suppl√©mentaire au SimpleMLP\n",
    "2. **Inspecter les gradients** : Apr√®s un backward(), afficher `linear_layer.weight.grad`\n",
    "3. **Essayer d'autres mod√®les** : Charger `prajjwal1/bert-tiny` (encore plus petit) et comparer\n",
    "4. **Batch processing** : Passer plusieurs phrases en m√™me temps et observer le batch size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}